{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Style Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing All Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflow.keras import mixed_precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GPU and Mixed Precision Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable mixed precision for improved performance and reduced memory usage (optional)\n",
    "mixed_precision.set_global_policy('mixed_float16')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Configure GPU memory growth to avoid pre-allocating all VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_WEIGHT = 1e4\n",
    "STYLE_WEIGHT = 1e-2\n",
    "TV_WEIGHT = 30\n",
    "STEPS = 1000\n",
    "LEARNING_RATE = 0.02\n",
    "MAX_DIM = 384"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Image Handling Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path, max_dim=MAX_DIM):\n",
    "    img = PIL.Image.open(path).convert('RGB')\n",
    "    img.thumbnail((max_dim, max_dim))\n",
    "    img = np.array(img)\n",
    "    img = tf.keras.applications.vgg19.preprocess_input(img)\n",
    "    return tf.expand_dims(tf.convert_to_tensor(img, dtype=tf.float32), 0)  # Ensure float32\n",
    "\n",
    "def deprocess_img(processed_img):\n",
    "    img = processed_img.numpy().squeeze()\n",
    "    img += [103.939, 116.779, 123.68]\n",
    "    img = img[:, :, ::-1]\n",
    "    return np.clip(img, 0, 255).astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \"\"\"Build VGG19 model with intermediate outputs for content and style layers.\"\"\"\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "    content_layer = 'block5_conv2'\n",
    "    style_layers = [f'block{i}_conv1' for i in range(1, 6)]\n",
    "    outputs = [vgg.get_layer(content_layer).output] + [vgg.get_layer(layer).output for layer in style_layers]\n",
    "    return tf.keras.Model(vgg.input, outputs)\n",
    "\n",
    "def gram_matrix(tensor):\n",
    "    \"\"\"Compute the Gram matrix for style representation.\"\"\"\n",
    "    channels = int(tensor.shape[-1])\n",
    "    a = tf.reshape(tensor, [-1, channels])\n",
    "    return tf.matmul(a, a, transpose_a=True) / tf.cast(tf.shape(a)[0], tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(content, generated):\n",
    "    \"\"\"Mean squared error between content and generated features.\"\"\"\n",
    "    return tf.reduce_mean(tf.square(content - generated))\n",
    "\n",
    "def style_loss(style, generated):\n",
    "    \"\"\"Mean squared error between style and generated Gram matrices.\"\"\"\n",
    "    return tf.reduce_mean(tf.square(style - generated))\n",
    "\n",
    "def total_variation_loss(image):\n",
    "    \"\"\"Total variation loss to promote image smoothness.\"\"\"\n",
    "    x_diff = image[:, 1:, :, :] - image[:, :-1, :, :]\n",
    "    y_diff = image[:, :, 1:, :] - image[:, :, :-1, :]\n",
    "    return tf.reduce_sum(tf.abs(x_diff)) + tf.reduce_sum(tf.abs(y_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load Images and Initialize Generated Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_img('content.jpg')\n",
    "style_image = load_img('style.jpg')\n",
    "generated_image = tf.Variable(content_image, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Extract Feature Targets from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "`x` and `y` must have the same dtype, got tf.float16 != tf.float32.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Cast targets to float32 for consistency\u001b[39;00m\n\u001b[0;32m      3\u001b[0m content_target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(model(content_image)[\u001b[38;5;241m0\u001b[39m], tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 4\u001b[0m style_targets \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mcast(gram_matrix(style_output), tf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m style_output \u001b[38;5;129;01min\u001b[39;00m model(style_image)[\u001b[38;5;241m1\u001b[39m:]]\n",
      "Cell \u001b[1;32mIn[19], line 4\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Cast targets to float32 for consistency\u001b[39;00m\n\u001b[0;32m      3\u001b[0m content_target \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcast(model(content_image)[\u001b[38;5;241m0\u001b[39m], tf\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 4\u001b[0m style_targets \u001b[38;5;241m=\u001b[39m [tf\u001b[38;5;241m.\u001b[39mcast(\u001b[43mgram_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstyle_output\u001b[49m\u001b[43m)\u001b[49m, tf\u001b[38;5;241m.\u001b[39mfloat32) \u001b[38;5;28;01mfor\u001b[39;00m style_output \u001b[38;5;129;01min\u001b[39;00m model(style_image)[\u001b[38;5;241m1\u001b[39m:]]\n",
      "Cell \u001b[1;32mIn[16], line 14\u001b[0m, in \u001b[0;36mgram_matrix\u001b[1;34m(tensor)\u001b[0m\n\u001b[0;32m     12\u001b[0m channels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(tensor\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m     13\u001b[0m a \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mreshape(tensor, [\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, channels])\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranspose_a\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\saura\\anaconda3\\envs\\common\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\saura\\anaconda3\\envs\\common\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:1517\u001b[0m, in \u001b[0;36m_truediv_python3\u001b[1;34m(x, y, name)\u001b[0m\n\u001b[0;32m   1515\u001b[0m y_dtype \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x_dtype \u001b[38;5;241m!=\u001b[39m y_dtype:\n\u001b[1;32m-> 1517\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`x` and `y` must have the same dtype, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1518\u001b[0m                   \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgot \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx_dtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m != \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_dtype\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1519\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1520\u001b[0m   dtype \u001b[38;5;241m=\u001b[39m _TRUEDIV_TABLE[x_dtype]\n",
      "\u001b[1;31mTypeError\u001b[0m: `x` and `y` must have the same dtype, got tf.float16 != tf.float32."
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "# Cast targets to float32 for consistency\n",
    "content_target = tf.cast(model(content_image)[0], tf.float32)\n",
    "style_targets = [tf.cast(gram_matrix(style_output), tf.float32) for style_output in model(style_image)[1:]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Optimizer Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Training Step (compiled with tf.function for performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(generated_image, content_target, style_targets):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = model(generated_image)\n",
    "        # Cast outputs to float32 for loss computation\n",
    "        generated_content = tf.cast(outputs[0], tf.float32)\n",
    "        generated_styles = [tf.cast(gram_matrix(style_output), tf.float32) for style_output in outputs[1:]]\n",
    "        \n",
    "        # Compute individual losses\n",
    "        c_loss = content_loss(content_target, generated_content)\n",
    "        s_loss = tf.add_n([\n",
    "            style_loss(style_target, generated_style)\n",
    "            for style_target, generated_style in zip(style_targets, generated_styles)\n",
    "        ]) / len(style_targets)\n",
    "        tv_loss = total_variation_loss(generated_image)\n",
    "        \n",
    "        total_loss = CONTENT_WEIGHT * c_loss + STYLE_WEIGHT * s_loss + TV_WEIGHT * tv_loss\n",
    "    gradients = tape.gradient(total_loss, generated_image)\n",
    "    optimizer.apply_gradients([(gradients, generated_image)])\n",
    "    return total_loss, c_loss, s_loss, tv_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "for step in range(STEPS):\n",
    "    total_loss, c_loss, s_loss, tv_loss = train_step(generated_image, content_target, style_targets)\n",
    "    if step % 100 == 0:\n",
    "        print(f\"Step {step}: Total Loss={total_loss:.2e}, Content Loss={c_loss:.2e}, Style Loss={s_loss:.2e}, TV Loss={tv_loss:.2e}\")\n",
    "print(f\"Total time: {time.time()-start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Display and Save the Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = deprocess_img(generated_image)\n",
    "plt.imshow(result)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "common",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
